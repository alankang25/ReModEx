{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c70a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For ChIP-seq data, download ENCODE metadata TSV file and filter it\n",
    "\"\"\"\n",
    "# user‐configurable parameters\n",
    "encode_tsv = \"/mnt/disk/home/akang/projects/manuscript/data/input/TF_files.tsv\"\n",
    "assay_type  = \"TF\"          # \"TF\" or \"histone\"\n",
    "min_peaks   = 1000\n",
    "n_threads   = 4\n",
    "output_dir  = \"../data/bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and functions\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def split_into_chunks(lst, n):\n",
    "    avg_len = len(lst)//n\n",
    "    remainder = len(lst)%n\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Each chunk gets an additional element if there's a remainder\n",
    "        end = start + avg_len + (1 if i < remainder else 0)\n",
    "        chunks.append(lst[start:end])\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "META_DIR = Path(\"../data/metadata\")\n",
    "META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_metadata(accession_list, metadata_dict, lock):\n",
    "    \"\"\"\n",
    "    Worker executed inside each thread.\n",
    "    Adds a lock‑protected write to the shared dict so it’s thread‑safe.\n",
    "    \"\"\"\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "    for i, accession in enumerate(accession_list):\n",
    "        if i % 10 == 0 and i:\n",
    "            time.sleep(1)\n",
    "\n",
    "        url = f\"https://www.encodeproject.org/files/{accession}\"\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {accession} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        (META_DIR / f\"{accession}.json\").write_text(json.dumps(data))\n",
    "\n",
    "        with lock:\n",
    "            metadata_dict[accession] = data\n",
    "\n",
    "\n",
    "def metadata_run(ENCODE_tsv, n):\n",
    "    \"\"\"\n",
    "    Downloads metadata JSONS from samples specified in the input TSV file from the ENCODE api.\n",
    "    \"\"\"\n",
    "    # Create the output directory \"../data/metadata\" . if already exists, delete all files in it\n",
    "    os.makedirs(\"../data/metadata\", exist_ok=True)\n",
    "    # Delete all files in the directory\n",
    "    for filename in os.listdir(\"../data/metadata\"):\n",
    "        file_path = os.path.join(\"../data/metadata\", filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file {file_path}: {e}\")\n",
    "\n",
    "    # Read the TSV file\n",
    "    ENCODE_df = pd.read_csv(ENCODE_tsv, sep='\\t', skiprows=1)\n",
    "    ENCODE_df = ENCODE_df[['Dataset', 'Accession', 'Target label']]\n",
    "\n",
    "    #split list into n groups for parallel processing\n",
    "    ENCODE_df = ENCODE_df['Accession'].tolist()\n",
    "    bed_chunks = split_into_chunks(ENCODE_df, n)\n",
    "    metadata_dict = {}\n",
    "    threads = []\n",
    "\n",
    "    # Create a lock for thread-safe access to the metadata_dict\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    for chunk in bed_chunks:\n",
    "        thread = threading.Thread(target=download_metadata, args=(chunk, metadata_dict, lock))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "def read_metadata_TF(min_peaks):\n",
    "    \"\"\"\n",
    "    Downloads BED file with highest frip for each target. Excludes files with less than threshold # of peaks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(\"../data/bed\", exist_ok=True)\n",
    "    bed_json_path = '../data/metadata/'\n",
    "\n",
    "    # Get data from JSON files\n",
    "    BED_frip_dict = {}\n",
    "    BED_dataset_dict = {}\n",
    "    BED_target_dict = {}\n",
    "    BED_reproducible_peaks = {}\n",
    "\n",
    "    for json_file in os.listdir(bed_json_path):\n",
    "        with open(bed_json_path + json_file) as f:\n",
    "            data = json.load(f)\n",
    "            if len(data['quality_metrics']) != 0:\n",
    "                if 'frip' in data['quality_metrics'][0]:\n",
    "                    frip = data['quality_metrics'][0]['frip']\n",
    "                elif len(data['quality_metrics']) == 2 and 'frip' in data['quality_metrics'][1]:\n",
    "                    frip = data['quality_metrics'][1]['frip']\n",
    "                else:\n",
    "                    frip = None\n",
    "            else:\n",
    "                frip = None\n",
    "\n",
    "            if len(data['quality_metrics']) != 0:\n",
    "                if 'reproducible_peaks' in data['quality_metrics'][0]:\n",
    "                    reproducible_peaks = data['quality_metrics'][0]['reproducible_peaks']\n",
    "                elif len(data['quality_metrics']) == 2 and 'reproducible_peaks' in data['quality_metrics'][1]:\n",
    "                    reproducible_peaks = data['quality_metrics'][1]['reproducible_peaks']\n",
    "                else:\n",
    "                    reproducible_peaks = None\n",
    "        \n",
    "                BED_frip_dict[json_file] = frip\n",
    "\n",
    "            BED_reproducible_peaks[json_file] = reproducible_peaks\n",
    "            dataset = data['dataset']\n",
    "            BED_dataset_dict[json_file] = dataset\n",
    "            target = data['target']['label']\n",
    "            BED_target_dict[json_file] = target\n",
    "    \n",
    "    # make into dataframe\n",
    "    #save to df\n",
    "    df_final = pd.DataFrame.from_dict(BED_frip_dict, orient='index', columns=['frip'])\n",
    "\n",
    "    #add reproducible_peaks column\n",
    "    df_final['reproducible_peaks'] = df_final.index\n",
    "    df_final['reproducible_peaks'] = df_final['reproducible_peaks'].map(BED_reproducible_peaks)\n",
    "\n",
    "    #add dataset column\n",
    "    df_final['dataset'] = df_final.index\n",
    "    df_final['dataset'] = df_final['dataset'].map(BED_dataset_dict)\n",
    "    #keep element 2 of dataset values using / as delimiter\n",
    "    df_final['dataset'] = df_final['dataset'].str.split('/').str[2]\n",
    "\n",
    "    #add target column\n",
    "    df_final['target'] = df_final.index\n",
    "    df_final['target'] = df_final['target'].map(BED_target_dict)\n",
    "\n",
    "    #add accession column which is index\n",
    "    df_final['accession'] = df_final.index\n",
    "    df_final['accession'] = df_final['accession'].str[:-5]\n",
    "\n",
    "    #reset index\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #order columns as accession, target, dataset, frip\n",
    "    df_final = df_final[['accession', 'target', 'dataset', 'frip', 'reproducible_peaks']]\n",
    "\n",
    "    #order by frip\n",
    "    df_final = df_final.sort_values(by='frip', ascending=False)\n",
    "\n",
    "    #keep first instance of target\n",
    "    df_final = df_final.drop_duplicates(subset='target', keep='first')\n",
    "\n",
    "    #keep only rows with reproducible_peaks > min_peaks\n",
    "    df_final = df_final[df_final['reproducible_peaks'] > min_peaks]\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def read_metadata_histone(min_peaks):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(\"../data/bed\", exist_ok=True)\n",
    "    bed_json_path = '../data/metadata/'\n",
    "\n",
    "    BED_frip_dict = {}\n",
    "    BED_dataset_dict = {}\n",
    "    BED_target_dict = {}\n",
    "\n",
    "    for json_file in os.listdir(bed_json_path):\n",
    "        with open(bed_json_path + json_file) as f:\n",
    "            data = json.load(f)\n",
    "            if len(data['quality_metrics']) != 0:\n",
    "                if 'frip' in data['quality_metrics'][0]:\n",
    "                    frip = data['quality_metrics'][0]['frip']\n",
    "                elif len(data['quality_metrics']) == 2 and 'frip' in data['quality_metrics'][1]:\n",
    "                    frip = data['quality_metrics'][1]['frip']\n",
    "                else:\n",
    "                    frip = None\n",
    "            else:\n",
    "                frip = None\n",
    "\n",
    "            dataset = data['dataset']\n",
    "            target = data['target']['label']\n",
    "\n",
    "            BED_frip_dict[json_file] = frip\n",
    "            BED_dataset_dict[json_file] = dataset\n",
    "            BED_target_dict[json_file] = target\n",
    "\n",
    "    #save frip to df\n",
    "    df_final = pd.DataFrame.from_dict(BED_frip_dict, orient='index', columns=['frip'])\n",
    "\n",
    "    #add target column\n",
    "    df_final['target'] = df_final.index\n",
    "    df_final['target'] = df_final['target'].map(BED_target_dict)\n",
    "\n",
    "    #add dataset column\n",
    "    df_final['dataset'] = df_final.index\n",
    "    df_final['dataset'] = df_final['dataset'].map(BED_dataset_dict)\n",
    "    df_final['dataset'] = df_final['dataset'].str.split('/').str[2]\n",
    "\n",
    "    #order by frip\n",
    "    df_final = df_final.sort_values(by='frip', ascending=False)\n",
    "\n",
    "    #keep first instance of target\n",
    "    df_final = df_final.drop_duplicates(subset='target', keep='first')\n",
    "\n",
    "    #parse index to get accession and store in column\n",
    "    df_final['accession'] = df_final.index\n",
    "    df_final['accession'] = df_final['accession'].str[:-5]\n",
    "\n",
    "    #reset index\n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #order columns as accession,target,dataset,frip\n",
    "    df_final = df_final[['accession', 'target', 'dataset', 'frip']]\n",
    "\n",
    "    #TODO: add peaks columns and check if > min_peaks\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "#download BED files and name them according to target format: \"https://www.encodeproject.org/files/ENCFF636FWF/@@download/ENCFF636FWF.bed.gz\"\n",
    "def download_BED_file_helper(target, accession):\n",
    "    os.system(f\"wget -q -O ../data/bed/{target}.bed.gz 'https://www.encodeproject.org/files/{accession}/@@download/{accession}.bed.gz'\")\n",
    "    print(f\"Downloaded {target}.bed\")\n",
    "\n",
    "def download_bed_file(accession_df, threads):\n",
    "    \"\"\"\n",
    "    Make accession dictionary with target as key and accession as value. Download bed files from ENCODE.\n",
    "    Logs each downloaded accession and target.\n",
    "    \"\"\"\n",
    "    # keep first 3 columns\n",
    "    accession_df = accession_df[['accession', 'target', 'dataset']]\n",
    "\n",
    "    # make dict (key = target, value = accession) from pandas dataframe\n",
    "    accession_dict = accession_df.set_index('target')['accession'].to_dict()\n",
    "\n",
    "    active = []  # list of currently running Thread objects\n",
    "    downloaded = []  # list to keep track of downloaded (target, accession)\n",
    "\n",
    "    def download_and_log(target, accession):\n",
    "        download_BED_file_helper(target, accession)\n",
    "        downloaded.append((target, accession))\n",
    "\n",
    "    for target, accession in accession_dict.items():\n",
    "        # start a new thread for this download\n",
    "        t = threading.Thread(target=download_and_log, args=(target, accession), daemon=True)\n",
    "        t.start()\n",
    "        active.append(t)\n",
    "\n",
    "        # if we've hit our limit, wait for the oldest to finish\n",
    "        if len(active) >= threads:\n",
    "            active[0].join()\n",
    "            active.pop(0)\n",
    "\n",
    "    # wait for any remaining threads\n",
    "    for t in active:\n",
    "        t.join()\n",
    "\n",
    "def logging(accession_df):\n",
    "    \"\"\"\n",
    "    Log the downloaded BED files and their FRiP scores.\n",
    "    \"\"\"\n",
    "    log_dir = Path(\"../data/log\")\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_file = log_dir / \"log.txt\"\n",
    "    i = 1\n",
    "    while log_file.exists():\n",
    "        log_file = log_dir / f\"log_{i}.txt\"\n",
    "        i += 1\n",
    "    with log_file.open(\"w\") as f:\n",
    "        for _, row in accession_df.iterrows():\n",
    "            f.write(f\"{row['accession']}\\t{row['target']}\\t{row['dataset']}\\t{row['frip']}\\n\")\n",
    "    print(f\"Log file created at {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
